{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'VnCoreNLP-1.2.jar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Kaisaac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:815\u001b[0m, in \u001b[0;36mmove\u001b[1;34m(src, dst, copy_function)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 815\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'VnCoreNLP-1.2.jar' -> 'C:/Users/Kaisaac/Documents/VSCode/ToxicChecker/ML/vncorenlp/VnCoreNLP-1.2.jar'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./vncorenlp\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Automatically download VnCoreNLP components from the original repository\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# and save them in some local working folder\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mpy_vncorenlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/Kaisaac/Documents/VSCode/ToxicChecker/ML/vncorenlp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m rdrsegmenter \u001b[38;5;241m=\u001b[39m py_vncorenlp\u001b[38;5;241m.\u001b[39mVnCoreNLP(annotators\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwseg\u001b[39m\u001b[38;5;124m\"\u001b[39m], save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Kaisaac/Documents/VSCode/ToxicChecker/ML/vncorenlp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÔng Nguyễn Khắc Chúc  đang làm việc tại Đại học Quốc gia Hà Nội. Bà Lan, vợ ông Chúc, cũng làm việc tại đây.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Kaisaac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py_vncorenlp\\vncorenlp.py:20\u001b[0m, in \u001b[0;36mdownload_model\u001b[1;34m(save_dir)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# jar\u001b[39;00m\n\u001b[0;32m     19\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.2.jar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVnCoreNLP-1.2.jar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/VnCoreNLP-1.2.jar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# wordsegmenter\u001b[39;00m\n\u001b[0;32m     22\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Kaisaac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:835\u001b[0m, in \u001b[0;36mmove\u001b[1;34m(src, dst, copy_function)\u001b[0m\n\u001b[0;32m    833\u001b[0m         rmtree(src)\n\u001b[0;32m    834\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 835\u001b[0m         \u001b[43mcopy_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m         os\u001b[38;5;241m.\u001b[39munlink(src)\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m real_dst\n",
      "File \u001b[1;32mc:\\Users\\Kaisaac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:434\u001b[0m, in \u001b[0;36mcopy2\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[0;32m    433\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[1;32m--> 434\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m copystat(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[1;32mc:\\Users\\Kaisaac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:254\u001b[0m, in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    252\u001b[0m     os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mreadlink(src), dst)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[0;32m    257\u001b[0m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'VnCoreNLP-1.2.jar'"
     ]
    }
   ],
   "source": [
    "import py_vncorenlp\n",
    "import os\n",
    "\n",
    "os.makedirs('./vncorenlp', exist_ok=True)\n",
    "# Automatically download VnCoreNLP components from the original repository\n",
    "# and save them in some local working folder\n",
    "py_vncorenlp.download_model(save_dir='C:/Users/Kaisaac/Documents/VSCode/ToxicChecker/ML/vncorenlp')\n",
    "\n",
    "rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir='C:/Users/Kaisaac/Documents/VSCode/ToxicChecker/ML/vncorenlp')\n",
    "\n",
    "\n",
    "text = \"Ông Nguyễn Khắc Chúc  đang làm việc tại Đại học Quốc gia Hà Nội. Bà Lan, vợ ông Chúc, cũng làm việc tại đây.\"\n",
    "output = rdrsegmenter.word_segment(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m      2\u001b[0m words \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      4\u001b[0m   word \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m      5\u001b[0m   words\u001b[38;5;241m.\u001b[39mextend(word)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "words = []\n",
    "for line in train_df['text']:\n",
    "  word = line.split()\n",
    "  words.extend(word)\n",
    "\n",
    "for line in val_df['text']:\n",
    "  word = line.split()\n",
    "  words.extend(word)\n",
    "\n",
    "for line in test_df['text']:\n",
    "  word = line.split()\n",
    "  words.extend(word)\n",
    "\n",
    "word_counts = Counter(words)\n",
    "# Sắp xếp từ điển theo số lượng từ xuất hiện từ lớn đến nhỏ\n",
    "sorted_word_counts = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_word_counts\n",
    "\n",
    "import pandas as pd\n",
    "teencode_df = pd.read_csv('/content/teencode.txt',names=['teencode','map'],sep='\\t',)\n",
    "teencode_list = teencode_df['teencode'].to_list()\n",
    "map_list = teencode_df['map'].to_list()\n",
    "\n",
    "def replace_teencode(line):\n",
    "    words = []\n",
    "    for word in line.strip().split():\n",
    "        if word in teencode_list:\n",
    "            index = teencode_list.index(word)\n",
    "            words.append(map_list[index])\n",
    "        else:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(replace_teencode)\n",
    "val_df['text'] = val_df['text'].apply(replace_teencode)\n",
    "test_df['text'] = test_df['text'].apply(replace_teencode)\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remove URLs https://www.\n",
    "    url_pattern = re.compile(r'https?://\\s+\\wwww\\.\\s+')\n",
    "    text = url_pattern.sub(r\" \", text)\n",
    "\n",
    "    # remove HTML Tags: <>\n",
    "    html_pattern = re.compile(r'<[^<>]+>')\n",
    "    text = html_pattern.sub(\" \", text)\n",
    "\n",
    "    # remove puncs and digits\n",
    "    replace_chars = list(string.punctuation + string.digits)\n",
    "    for char in replace_chars:\n",
    "        text = text.replace(char, \" \")\n",
    "\n",
    "    # remove emoji\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "        u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U0001F1F2\"\n",
    "        u\"\\U0001F1F4\"\n",
    "        u\"\\U0001F620\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r\" \", text)\n",
    "\n",
    "    # normalize whitespace\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    # lowercasing\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "train_df['text'] = [preprocess_text(row['text']) for index, row in train_df.iterrows()]\n",
    "val_df['text'] = [preprocess_text(row['text']) for index, row in val_df.iterrows()]\n",
    "test_df['text'] = [preprocess_text(row['text']) for index, row in test_df.iterrows()]\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lambda x: ' '.join(rdrsegmenter.word_segment(x)))\n",
    "val_df['text'] = val_df['text'].apply(lambda x: ' '.join(rdrsegmenter.word_segment(x)))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: ' '.join(rdrsegmenter.word_segment(x)))\n",
    "\n",
    "stopwords = ['là', 'thì']\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line.strip().split():\n",
    "        if word not in stopwords:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(remove_stopwords)\n",
    "val_df['text'] = val_df['text'].apply(remove_stopwords)\n",
    "test_df['text'] = test_df['text'].apply(remove_stopwords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
